{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lightweight Fine-Tuning Project\n",
    "TODO: In this cell, describe your choices for each of the following\n",
    "\n",
    "PEFT technique:\n",
    "Model:\n",
    "Evaluation approach:\n",
    "Fine-tuning dataset:\n",
    "Loading and Evaluating a Foundation Model\n",
    "TODO: In the cells below, load your chosen pre-trained Hugging Face model and evaluate its performance prior to fine-tuning. This step includes loading an appropriate tokenizer and dataset.\n",
    "\n",
    "Performing Parameter-Efficient Fine-Tuning\n",
    "TODO: In the cells below, create a PEFT model from your loaded model, run a training loop, and save the PEFT model weights.\n",
    "\n",
    "Performing Inference with a PEFT Model\n",
    "TODO: In the cells below, load the saved PEFT model weights and evaluate the performance of the trained PEFT model. Be sure to compare the results to the results from prior to fine-tuning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\shiva\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\shiva\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\shiva\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Map: 100%|██████████| 150/150 [00:00<00:00, 977.74 examples/s]\n",
      "Map: 100%|██████████| 150/150 [00:00<00:00, 2948.09 examples/s]\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "                                               \n",
      " 33%|███▎      | 19/57 [03:17<03:06,  4.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6833470463752747, 'eval_accuracy': 0.64, 'eval_runtime': 90.8698, 'eval_samples_per_second': 1.651, 'eval_steps_per_second': 0.209, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \n",
      " 67%|██████▋   | 38/57 [07:33<01:46,  5.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.669525682926178, 'eval_accuracy': 0.6866666666666666, 'eval_runtime': 139.1511, 'eval_samples_per_second': 1.078, 'eval_steps_per_second': 0.137, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \n",
      "100%|██████████| 57/57 [11:30<00:00,  5.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5093522071838379, 'eval_accuracy': 0.76, 'eval_runtime': 100.0462, 'eval_samples_per_second': 1.499, 'eval_steps_per_second': 0.19, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 57/57 [11:38<00:00, 12.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 698.5293, 'train_samples_per_second': 0.644, 'train_steps_per_second': 0.082, 'train_loss': 0.5029377184416118, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19/19 [01:34<00:00,  4.97s/it]\n",
      " 20%|██        | 15/75 [08:52<31:48, 31.80s/it]\n",
      " 20%|██        | 15/75 [10:37<31:48, 31.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6064311265945435, 'eval_accuracy': 0.74, 'eval_runtime': 105.249, 'eval_samples_per_second': 1.425, 'eval_steps_per_second': 0.143, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 30/75 [18:33<23:21, 31.14s/it]  \n",
      " 40%|████      | 30/75 [20:17<23:21, 31.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6371561884880066, 'eval_accuracy': 0.82, 'eval_runtime': 103.9243, 'eval_samples_per_second': 1.443, 'eval_steps_per_second': 0.144, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 45/75 [28:14<16:24, 32.83s/it]\n",
      " 60%|██████    | 45/75 [29:56<16:24, 32.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.0088210105895996, 'eval_accuracy': 0.72, 'eval_runtime': 101.3506, 'eval_samples_per_second': 1.48, 'eval_steps_per_second': 0.148, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 60/75 [37:46<07:55, 31.70s/it]\n",
      " 80%|████████  | 60/75 [39:29<07:55, 31.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.9266982078552246, 'eval_accuracy': 0.8, 'eval_runtime': 103.0859, 'eval_samples_per_second': 1.455, 'eval_steps_per_second': 0.146, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 75/75 [46:52<00:00, 28.66s/it]\n",
      "100%|██████████| 75/75 [48:37<00:00, 28.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.9356085062026978, 'eval_accuracy': 0.7933333333333333, 'eval_runtime': 104.5756, 'eval_samples_per_second': 1.434, 'eval_steps_per_second': 0.143, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 75/75 [48:54<00:00, 39.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 2934.4925, 'train_samples_per_second': 0.256, 'train_steps_per_second': 0.026, 'train_loss': 0.09685407638549805, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [01:33<00:00,  6.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base Model Evaluation:\n",
      "{'eval_loss': 0.5093522071838379, 'eval_accuracy': 0.76, 'eval_runtime': 99.2006, 'eval_samples_per_second': 1.512, 'eval_steps_per_second': 0.192, 'epoch': 3.0}\n",
      "\n",
      "Fine-Tuned Model Evaluation:\n",
      "{'eval_loss': 0.6064311265945435, 'eval_accuracy': 0.74, 'eval_runtime': 98.7266, 'eval_samples_per_second': 1.519, 'eval_steps_per_second': 0.152, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, DataCollatorWithPadding, TrainingArguments, Trainer, AutoModelForSequenceClassification\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from peft import LoraConfig, LoraModel\n",
    "\n",
    "\n",
    "splits = [\"train\", \"test\"]\n",
    "dataSet = {split: data for split, data in zip(splits, load_dataset(\"rotten_tomatoes\", split=splits))}\n",
    "\n",
    "# Thin out the dataset for quicker execution\n",
    "for split in splits:\n",
    "    dataSet[split] = dataSet[split].shuffle(seed=42).select(range(150))\n",
    "    \n",
    "'''\n",
    "\n",
    "\n",
    "def preprocessFunction(examples):\n",
    "    outputs = tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
    "    return {\n",
    "        \"input_ids\": outputs[\"input_ids\"].squeeze(),\n",
    "        \"attention_mask\": outputs[\"attention_mask\"].squeeze(),\n",
    "        \"labels\": torch.tensor(examples[\"label\"])\n",
    "    }    \n",
    "    \n",
    "tokenized_ds = {}\n",
    "for split in splits:\n",
    "    tokenized_ds[split] = ds[split].map(preprocess_function, batched=True)\n",
    "    tokenized_ds[split] = tokenized_ds[split].rename_column(\"label\", \"original_label\")\n",
    "    tokenized_ds[split] = tokenized_ds[split].with_format(\"torch\")\n",
    "'''\n",
    "# Pre-process the dataset\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "def preprocessFunction(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "\n",
    "tokenizedDataSet = {split: dataSet[split].map(preprocessFunction, batched=True) for split in splits}\n",
    "\n",
    "# Initialize the base model\n",
    "baseModel = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"distilbert-base-uncased\",\n",
    "    num_labels=2,\n",
    "    id2label={0: \"NEGATIVE\", 1: \"POSITIVE\"},\n",
    "    label2id={\"NEGATIVE\": 0, \"POSITIVE\": 1},\n",
    ")\n",
    "\n",
    "\n",
    "# Freeze the model parameters\n",
    "for param in baseModel.base_model.parameters():\n",
    "    param.requires_grad = False\n",
    "    \n",
    "\n",
    "def computeMetrics(evalPred):\n",
    "    predictions, labels = evalPred\n",
    "    preds = np.argmax(predictions, axis=1)\n",
    "    return {\"accuracy\": (preds == labels).mean()}\n",
    "\n",
    "\n",
    "# Training the base model\n",
    "trainingArgsBase = TrainingArguments(\n",
    "    output_dir=\"./data/sentiment_analysis_base_model\",\n",
    "    learning_rate=3e-3,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    ")\n",
    "\n",
    "trainerBase = Trainer(\n",
    "    model=baseModel,\n",
    "    args=trainingArgsBase,\n",
    "    train_dataset=tokenizedDataSet[\"train\"],\n",
    "    eval_dataset=tokenizedDataSet[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=DataCollatorWithPadding(tokenizer=tokenizer),\n",
    "    compute_metrics=computeMetrics,\n",
    ")\n",
    "\n",
    "trainerBase.train()\n",
    "\n",
    "# Evaluate the base model\n",
    "baseModelEvaluation = trainerBase.evaluate()\n",
    "\n",
    "\"\"\"\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1,\n",
    "    target_modules=[\n",
    "        \"distilbert.transformer.layer.0.attention.q_lin\",\n",
    "        \"distilbert.transformer.layer.0.attention.k_lin\",\n",
    "        \"distilbert.transformer.layer.0.attention.v_lin\",\n",
    "        \"distilbert.transformer.layer.1.attention.q_lin\",\n",
    "        \"distilbert.transformer.layer.1.attention.k_lin\",\n",
    "        \"distilbert.transformer.layer.1.attention.v_lin\",\n",
    "        \"distilbert.transformer.layer.5.attention.q_lin\",\n",
    "        \"distilbert.transformer.layer.5.attention.k_lin\",\n",
    "        \"distilbert.transformer.layer.5.attention.v_lin\",\n",
    "    ],\n",
    "    fan_in_fan_out=True\n",
    ")\n",
    "\"\"\"\n",
    "# Unfreeze the model parameters for fine-tuning\n",
    "for param in baseModel.parameters():\n",
    "    param.requires_grad = True\n",
    "    \n",
    "'''\n",
    "# Wrap the model with LoRA\n",
    "peft_model = LoraModel(\n",
    "    base_model,\n",
    "    lora_config,\n",
    "    adapter_name=\"lora\"\n",
    ")\n",
    "'''\n",
    "# Training the fine-tuned model\n",
    "trainingArgsPeft = TrainingArguments(\n",
    "    output_dir=\"./data/sentiment_analysis_peft_model\",\n",
    "    learning_rate=3e-5,\n",
    "    per_device_train_batch_size=10,\n",
    "    per_device_eval_batch_size=10,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    ")\n",
    "\n",
    "trainerPeft = Trainer(\n",
    "    model=baseModel,\n",
    "    args=trainingArgsPeft,\n",
    "    train_dataset=tokenizedDataSet[\"train\"],\n",
    "    eval_dataset=tokenizedDataSet[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=DataCollatorWithPadding(tokenizer=tokenizer),\n",
    "    compute_metrics=computeMetrics,\n",
    ")\n",
    "\n",
    "trainerPeft.train()\n",
    "\n",
    "# Evaluate the fine-tuned model\n",
    "peftModelEvaluation = trainerPeft.evaluate()\n",
    "\n",
    "# Compare the results \n",
    "print(\"Base Model Evaluation:\")\n",
    "print(baseModelEvaluation)\n",
    "\n",
    "print(\"\\nFine-Tuned Model Evaluation:\")\n",
    "print(peftModelEvaluation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Base Model Evaluation:\n",
    "\n",
    "Evaluation Loss: 0.5094\n",
    "Accuracy: 0.76\n",
    "Runtime: 99.2006 seconds\n",
    "Samples Per Second: 1.512\n",
    "Steps Per Second: 0.192\n",
    "Epochs: 3.0\n",
    "\n",
    "Fine-Tuned Model Evaluation:\n",
    "\n",
    "Evaluation Loss: 0.6064\n",
    "Accuracy: 0.74\n",
    "Runtime: 98.7266 seconds\n",
    "Samples Per Second: 1.519\n",
    "Steps Per Second: 0.152\n",
    "Epochs: 5.0\n",
    "\n",
    "\n",
    "Analysis:\n",
    "\n",
    "Evaluation Loss: The Fine-Tuned Model (0.6064) has a higher evaluation loss than the Base Model (0.5094). When the loss is smaller, the Base Model outperforms the Fine-Tuned Model in terms of test data fit.\n",
    "\n",
    "Accuracy: The Base Model outperformed the Fine-Tuned Model (0.74) in accuracy, coming in at 0.76. A higher accuracy rate indicates that a greater percentage of the test samples were accurately predicted by the Base Model.\n",
    "\n",
    "Processing speed and runtime:\n",
    "    The Base Model's and the Fine-Tuned Model's evaluation runtimes are comparable, clocking in at 99.2006 and 98.7266 seconds, respectively.\n",
    "\n",
    "    Compared to the Base Model (1.511), the Fine-Tuned Model processed 1.519 samples per second, which is a modest increase. However, the Fine-Tuned Model (0.152) processed fewer steps per second (0.152) than the Base Model (0.192), suggesting a possible discrepancy in the evaluation process's complexity or effectiveness.\n",
    "\n",
    "    \n",
    "\n",
    "Conclusion: \n",
    "\n",
    "The Base Model fared better in terms of evaluation loss and accuracy than the Fine-Tuned Model, in contrast to the first example when fine-tuning produced notable gains. It appears that the Base Model was more successful in identifying patterns in the test data for sentiment analysis using the Rotten Tomatoes dataset, as seen by its lower loss and greater accuracy. This result suggests that the particular fine-tuning procedure used in this instance did not improve the model's performance and might have resulted in overfitting or less-than-ideal modifications. Therefore, in order to find possible improvements, more research into the fine-tuning parameters and procedures is advised.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
